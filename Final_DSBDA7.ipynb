{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q3EbPmk9TrYY"
      },
      "outputs": [],
      "source": [
        "sentence1 = \"I will walk 500 miles and I would walk 500 more. Just to be the man who walks a thousand miles to fall down at your door!\"\n",
        "sentence2 = \"I played the play playfully as the players were playing in the play with playfullness\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLTK\n",
        "The Natural Language Toolkit (NLTK) is a platform used for building Python programs that work with human language data for applying in statistical natural language processing (NLP)."
      ],
      "metadata": {
        "id": "de2qR7KVXuKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tokenization\n",
        "Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens."
      ],
      "metadata": {
        "id": "LruHcgN8U1Mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize , sent_tokenize\n",
        "# to use word_tokenize and sent_tokenize. we must have to download punkt from nltk\n",
        "nltk.download('punkt') #punkt is sentence boundary detection algorithm "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzZY9aTYU3ze",
        "outputId": "1ef22fd1-c37d-4453-fa70-77ed3d81668e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(sentence1) #split a document or paragraph into sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EsTwPEKVH2P",
        "outputId": "7c04f20c-002c-417e-df43-27e3bf1de0f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I will walk 500 miles and I would walk 500 more.',\n",
              " 'Just to be the man who walks a thousand miles to fall down at your door!']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token1=word_tokenize(sentence1) #split a sentence into tokens or words\n",
        "token1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m92CbjP5VUd2",
        "outputId": "180a5025-6218-4133-800a-a769a8457d6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'will',\n",
              " 'walk',\n",
              " '500',\n",
              " 'miles',\n",
              " 'and',\n",
              " 'I',\n",
              " 'would',\n",
              " 'walk',\n",
              " '500',\n",
              " 'more',\n",
              " '.',\n",
              " 'Just',\n",
              " 'to',\n",
              " 'be',\n",
              " 'the',\n",
              " 'man',\n",
              " 'who',\n",
              " 'walks',\n",
              " 'a',\n",
              " 'thousand',\n",
              " 'miles',\n",
              " 'to',\n",
              " 'fall',\n",
              " 'down',\n",
              " 'at',\n",
              " 'your',\n",
              " 'door',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token2=word_tokenize(sentence2)\n",
        "token2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlzXe_KdVXz-",
        "outputId": "20bb089d-e48d-4833-ba93-495ee0343af3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'played',\n",
              " 'the',\n",
              " 'play',\n",
              " 'playfully',\n",
              " 'as',\n",
              " 'the',\n",
              " 'players',\n",
              " 'were',\n",
              " 'playing',\n",
              " 'in',\n",
              " 'the',\n",
              " 'play',\n",
              " 'with',\n",
              " 'playfullness']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(sentence2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0w-2pXuVihf",
        "outputId": "015a70f1-fd7a-4576-f88b-627b4c3b8d45"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I played the play playfully as the players were playing in the play with playfullness']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part of Speech (PoS) Tagging\n",
        "A task of labelling each word in a sentence with its appropriate part of speech. Parts of speech include nouns, verb, adverbs, adjectives, pronouns, conjunction and their sub-categories.\n"
      ],
      "metadata": {
        "id": "1C5ipBGrWvO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "# to use pos tagger we must have to download averaged_perceptron_tagger from nltk.download()\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir2GjM8GVlxW",
        "outputId": "bb0dc678-6688-4319-a164-8046ecf2b59d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_token1=pos_tag(token1)\n",
        "tagged_token1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gYuwnikWuc2",
        "outputId": "c949662d-4b4a-4a6b-d0e5-c4865e209666"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('will', 'MD'),\n",
              " ('walk', 'VB'),\n",
              " ('500', 'CD'),\n",
              " ('miles', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('I', 'PRP'),\n",
              " ('would', 'MD'),\n",
              " ('walk', 'VB'),\n",
              " ('500', 'CD'),\n",
              " ('more', 'JJR'),\n",
              " ('.', '.'),\n",
              " ('Just', 'NNP'),\n",
              " ('to', 'TO'),\n",
              " ('be', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('man', 'NN'),\n",
              " ('who', 'WP'),\n",
              " ('walks', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('thousand', 'NN'),\n",
              " ('miles', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('fall', 'VB'),\n",
              " ('down', 'RP'),\n",
              " ('at', 'IN'),\n",
              " ('your', 'PRP$'),\n",
              " ('door', 'NN'),\n",
              " ('!', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_token2=pos_tag(token2)\n",
        "tagged_token2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6nPafBuXYXW",
        "outputId": "39105935-c5c1-466e-9b9e-90c9423125bd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('played', 'VBD'),\n",
              " ('the', 'DT'),\n",
              " ('play', 'NN'),\n",
              " ('playfully', 'RB'),\n",
              " ('as', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('players', 'NNS'),\n",
              " ('were', 'VBD'),\n",
              " ('playing', 'VBG'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('play', 'NN'),\n",
              " ('with', 'IN'),\n",
              " ('playfullness', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#StopWords\n",
        "The words which are generally filtered out before processing a natural language are called stop words. These are actually the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text."
      ],
      "metadata": {
        "id": "wxZrv4TcXer3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "# to get all the stop words of any language. we first have to download it from nltk.download('stopwords)\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gebRLjkUX1we",
        "outputId": "7ada897b-c469-4cbe-d5e8-bd2272d5bc0f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "3-7YMqTOX4Mq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_token1 = []\n",
        "clean_token2 = []\n",
        "for word in token1:\n",
        "  if word not in stop_words:\n",
        "    clean_token1.append(word)\n",
        "\n",
        "for word in token2:\n",
        "  if word not in stop_words:\n",
        "    clean_token2.append(word)"
      ],
      "metadata": {
        "id": "QMevvaFUYCOg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_token1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWyp250RYeLn",
        "outputId": "e384a763-9a01-4ab5-9f75-371b2dc60c2f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'walk',\n",
              " '500',\n",
              " 'miles',\n",
              " 'I',\n",
              " 'would',\n",
              " 'walk',\n",
              " '500',\n",
              " '.',\n",
              " 'Just',\n",
              " 'man',\n",
              " 'walks',\n",
              " 'thousand',\n",
              " 'miles',\n",
              " 'fall',\n",
              " 'door',\n",
              " '!']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_token2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xELLv-5YhJT",
        "outputId": "ae8149f3-94e0-43b1-99ff-1873d7f6d776"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'played',\n",
              " 'play',\n",
              " 'playfully',\n",
              " 'players',\n",
              " 'playing',\n",
              " 'play',\n",
              " 'playfullness']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Stemming\n",
        "It is normalization technique where the tokenized words are shortened to avoid the redundancy"
      ],
      "metadata": {
        "id": "Yhdd_h7QYm_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed1 = [stemmer.stem(word) for word in token1]\n",
        "print(\" \".join(stemmed1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibb_vHWeYsD5",
        "outputId": "23c429eb-391e-40ea-8da7-9a121e233db4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i will walk 500 mile and i would walk 500 more . just to be the man who walk a thousand mile to fall down at your door !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed2 = [stemmer.stem(word) for word in token2]\n",
        "print(\" \".join(stemmed2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxYeEzMVZGwQ",
        "outputId": "cc318186-6728-4de6-a471-98a6efe5467c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i play the play play as the player were play in the play with playful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization\n",
        "Considers the context and converts the word to its meaningful base form"
      ],
      "metadata": {
        "id": "_qIfdywOZPHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of lemmatization is same as that of stemming but overcomes the drawbacks of stemming. In stemming, for some words, it may not give may not give meaningful representation such as “Histori”. Here, lemmatization comes into picture as it gives meaningful word."
      ],
      "metadata": {
        "id": "KPpTk7grZ0-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# to use lemmatizer, we must donwload wordnet from nltk.download('wordnet)\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnjDXZFnZ-Ma",
        "outputId": "35306536-caa8-4b70-8ec1-f20636db4d05"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_output1 = [lemmatizer.lemmatize(word) for word in token1]\n",
        "lemmatized_output2 = [lemmatizer.lemmatize(word) for word in token2]\n",
        "print(\" \".join(lemmatized_output1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3BoeW53Z-2-",
        "outputId": "c2d63e3b-7e2f-4cae-a36f-07c4d97f2e89"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I will walk 500 mile and I would walk 500 more . Just to be the man who walk a thousand mile to fall down at your door !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\" \".join(lemmatized_output2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLwY4OHxaDfz",
        "outputId": "932a08a5-0b33-475b-ab60-a2d23a853419"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I played the play playfully a the player were playing in the play with playfullness\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Term Frequency (TF)\n",
        "It calculates the frequency of each word in a document. It represents how often a word appears in the document"
      ],
      "metadata": {
        "id": "k9T501YJaICK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inverse Document Frequency (IDF) \n",
        "Itmeasures the importance of a word in a collection of documents. It penalizes common words and gives more weight to rare words."
      ],
      "metadata": {
        "id": "6yXliziOjKLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Combine the sentences into a single list\n",
        "sentences = [\n",
        "    \"I will walk 500 miles and I would walk 500 more. Just to be the man who walks a thousand miles to fall down at your door!\",\n",
        "    \"I played the play playfully as the players were playing in the play with playfullness\"\n",
        "]\n",
        "\n",
        "# Create the TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the sentences\n",
        "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = vectorizer.get_feature_names_out() \n",
        "\n",
        "# Print the TF-IDF matrix\n",
        "print(\"Term Frequency-Inverse Document Frequency:\")\n",
        "for i in range(len(sentences)):\n",
        "    print(f\"\\nSentence {i+1}:\")\n",
        "    feature_index = tfidf_matrix[i].nonzero()[1]\n",
        "    tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
        "    for feature_idx, tfidf_score in tfidf_scores:\n",
        "        print(f\"Term: {feature_names[feature_idx]}, TF-IDF: {tfidf_score:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssq_Y5kuaIzm",
        "outputId": "b1859771-3576-489e-a924-6507d66cd26d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term Frequency-Inverse Document Frequency:\n",
            "\n",
            "Sentence 1:\n",
            "Term: door, TF-IDF: 0.1782\n",
            "Term: your, TF-IDF: 0.1782\n",
            "Term: at, TF-IDF: 0.1782\n",
            "Term: down, TF-IDF: 0.1782\n",
            "Term: fall, TF-IDF: 0.1782\n",
            "Term: thousand, TF-IDF: 0.1782\n",
            "Term: walks, TF-IDF: 0.1782\n",
            "Term: who, TF-IDF: 0.1782\n",
            "Term: man, TF-IDF: 0.1782\n",
            "Term: the, TF-IDF: 0.1268\n",
            "Term: be, TF-IDF: 0.1782\n",
            "Term: to, TF-IDF: 0.3563\n",
            "Term: just, TF-IDF: 0.1782\n",
            "Term: more, TF-IDF: 0.1782\n",
            "Term: would, TF-IDF: 0.1782\n",
            "Term: and, TF-IDF: 0.1782\n",
            "Term: miles, TF-IDF: 0.3563\n",
            "Term: 500, TF-IDF: 0.3563\n",
            "Term: walk, TF-IDF: 0.3563\n",
            "Term: will, TF-IDF: 0.1782\n",
            "\n",
            "Sentence 2:\n",
            "Term: playfullness, TF-IDF: 0.2387\n",
            "Term: with, TF-IDF: 0.2387\n",
            "Term: in, TF-IDF: 0.2387\n",
            "Term: playing, TF-IDF: 0.2387\n",
            "Term: were, TF-IDF: 0.2387\n",
            "Term: players, TF-IDF: 0.2387\n",
            "Term: as, TF-IDF: 0.2387\n",
            "Term: playfully, TF-IDF: 0.2387\n",
            "Term: play, TF-IDF: 0.4773\n",
            "Term: played, TF-IDF: 0.2387\n",
            "Term: the, TF-IDF: 0.5094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BYZmLYRxkkdj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}